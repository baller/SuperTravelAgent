# âš™ï¸ é…ç½®å‚è€ƒ

æœ¬æ–‡æ¡£æä¾› Sage å¤šæ™ºèƒ½ä½“æ¡†æ¶é…ç½®çš„å®Œæ•´å‚è€ƒã€‚

## ğŸ“‹ ç›®å½•

- [é…ç½®å±‚æ¬¡ç»“æ„](#-é…ç½®å±‚æ¬¡ç»“æ„)
- [ç¯å¢ƒå˜é‡](#-ç¯å¢ƒå˜é‡)
- [é…ç½®æ–‡ä»¶](#-é…ç½®æ–‡ä»¶)
- [å‘½ä»¤è¡Œå‚æ•°](#-å‘½ä»¤è¡Œå‚æ•°)
- [è¿è¡Œæ—¶é…ç½®](#-è¿è¡Œæ—¶é…ç½®)
- [æ¨¡å‹é…ç½®](#-æ¨¡å‹é…ç½®)
- [æ™ºèƒ½ä½“é…ç½®](#-æ™ºèƒ½ä½“é…ç½®)
- [å·¥å…·é…ç½®](#-å·¥å…·é…ç½®)
- [é«˜çº§è®¾ç½®](#-é«˜çº§è®¾ç½®)

## ğŸ—ï¸ é…ç½®å±‚æ¬¡ç»“æ„

Sage ä½¿ç”¨åˆ†å±‚é…ç½®ç³»ç»Ÿï¼Œè®¾ç½®æŒ‰ä»¥ä¸‹é¡ºåºåº”ç”¨ï¼ˆåé¢çš„å€¼è¦†ç›–å‰é¢çš„å€¼ï¼‰ï¼š

```mermaid
graph TD
    A[é»˜è®¤å€¼] --> B[é…ç½®æ–‡ä»¶]
    B --> C[ç¯å¢ƒå˜é‡]
    C --> D[å‘½ä»¤è¡Œå‚æ•°]
    D --> E[è¿è¡Œæ—¶æ›´æ–°]
    
    style A fill:#f0f0f0
    style E fill:#ffeb3b
```

1. **é»˜è®¤å€¼**: ä»£ç ä¸­çš„å†…ç½®é»˜è®¤å€¼
2. **é…ç½®æ–‡ä»¶**: YAML/JSON é…ç½®æ–‡ä»¶
3. **ç¯å¢ƒå˜é‡**: ç³»ç»Ÿç¯å¢ƒå˜é‡
4. **å‘½ä»¤è¡Œå‚æ•°**: å‘½ä»¤è¡Œå‚æ•°
5. **è¿è¡Œæ—¶æ›´æ–°**: æ‰§è¡ŒæœŸé—´çš„åŠ¨æ€æ›´æ–°

## ğŸŒ ç¯å¢ƒå˜é‡

### æ ¸å¿ƒè®¾ç½®

| å˜é‡ | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `OPENAI_API_KEY` | string | None | æ¨¡å‹è®¿é—®çš„ OpenAI API å¯†é’¥ |
| `SAGE_DEBUG` | boolean | false | å¯ç”¨è°ƒè¯•æ—¥å¿— |
| `SAGE_ENVIRONMENT` | string | "production" | è¿è¡Œæ—¶ç¯å¢ƒ (development/production) |
| `SAGE_LOG_LEVEL` | string | "INFO" | æ—¥å¿—çº§åˆ« (DEBUG/INFO/WARNING/ERROR) |
| `SAGE_CONFIG_PATH` | string | "./config" | é…ç½®æ–‡ä»¶è·¯å¾„ |

### æ¨¡å‹è®¾ç½®

| å˜é‡ | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `SAGE_MODEL_NAME` | string | "gpt-3.5-turbo" | é»˜è®¤æ¨¡å‹åç§° |
| `SAGE_BASE_URL` | string | None | è‡ªå®šä¹‰ API åŸºç¡€ URL |
| `SAGE_MAX_TOKENS` | integer | 4096 | æ¯ä¸ªè¯·æ±‚çš„æœ€å¤§æ ‡è®°æ•° |
| `SAGE_TEMPERATURE` | float | 0.7 | æ¨¡å‹æ¸©åº¦ (0-1) |
| `SAGE_TIMEOUT` | integer | 60 | è¯·æ±‚è¶…æ—¶ç§’æ•° |

### æ™ºèƒ½ä½“è®¾ç½®

| å˜é‡ | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `SAGE_MAX_ITERATIONS` | integer | 10 | æœ€å¤§æ™ºèƒ½ä½“è¿­ä»£æ¬¡æ•° |
| `SAGE_DEEP_THINKING` | boolean | true | é»˜è®¤å¯ç”¨ä»»åŠ¡åˆ†æ |
| `SAGE_DEEP_RESEARCH` | boolean | true | é»˜è®¤åœ¨æµå¼æ¨¡å¼ä¸­å¯ç”¨æ·±åº¦ç ”ç©¶ |
| `SAGE_SUMMARY_MODE` | boolean | true | é»˜è®¤ç”Ÿæˆæ€»ç»“ |
| `SAGE_STREAMING` | boolean | false | é»˜è®¤å¯ç”¨æµå¼è¾“å‡º |

### å·¥å…·è®¾ç½®

| å˜é‡ | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `SAGE_TOOLS_PATH` | string | "./agents/tool" | å·¥å…·ç›®å½•è·¯å¾„ |
| `SAGE_MCP_SERVERS_PATH` | string | "./mcp_servers" | MCP æœåŠ¡å™¨é…ç½®è·¯å¾„ |
| `SAGE_TOOL_TIMEOUT` | integer | 30 | å·¥å…·æ‰§è¡Œè¶…æ—¶ |
| `SAGE_MAX_CONCURRENT_TOOLS` | integer | 5 | æœ€å¤§å¹¶è¡Œå·¥å…·æ‰§è¡Œæ•° |

### ç¤ºä¾‹ .env æ–‡ä»¶

```bash
# Sage é…ç½®çš„ .env æ–‡ä»¶

# API é…ç½®
OPENAI_API_KEY=sk-your-openai-api-key-here
SAGE_BASE_URL=https://api.openai.com/v1

# æ¨¡å‹è®¾ç½®
SAGE_MODEL_NAME=gpt-4
SAGE_MAX_TOKENS=8192
SAGE_TEMPERATURE=0.3

# æ™ºèƒ½ä½“è¡Œä¸º
SAGE_DEEP_THINKING=true
SAGE_DEEP_RESEARCH=true
SAGE_SUMMARY_MODE=true
SAGE_MAX_ITERATIONS=15

# å¼€å‘è®¾ç½®
SAGE_DEBUG=true
SAGE_ENVIRONMENT=development
SAGE_LOG_LEVEL=DEBUG

# å·¥å…·è®¾ç½®
SAGE_TOOLS_PATH=/custom/tools:/default/tools
SAGE_TOOL_TIMEOUT=60
```

## ğŸ“ é…ç½®æ–‡ä»¶

### ä¸»é…ç½® (config/settings.yaml)

```yaml
# Sage å¤šæ™ºèƒ½ä½“æ¡†æ¶é…ç½®

model:
  name: "gpt-4"
  base_url: "https://api.openai.com/v1"
  max_tokens: 4096
  temperature: 0.7
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0
  timeout: 60
  retry_count: 3

agent:
  max_iterations: 10
  deep_thinking: true
  deep_research: true
  summary_mode: true
  streaming: false
  task_analysis:
    enabled: true
    max_depth: 3
  planning:
    enabled: true
    max_subtasks: 20
  execution:
    parallel_tools: true
    max_concurrent: 5
  observation:
    enabled: true
    feedback_threshold: 0.8

tool:
  directories:
    - "./agents/tool"
    - "./custom_tools"
  timeout: 30
  max_concurrent: 5
  retry_count: 2
  mcp_servers:
    config_path: "./mcp_servers/mcp_setting.json"
    auto_connect: true

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/sage.log"
  rotate: true
  max_size: "10MB"
  backup_count: 5

debug:
  enabled: false
  profile: false
  trace_calls: false
  save_conversations: false
```

### æ¨¡å‹ç‰¹å®šé…ç½® (config/models.yaml)

```yaml
# æ¨¡å‹ç‰¹å®šé…ç½®

models:
  gpt-4:
    max_tokens: 8192
    temperature: 0.3
    best_for: ["å¤æ‚æ¨ç†", "ä»£ç ç”Ÿæˆ"]
    
  gpt-3.5-turbo:
    max_tokens: 4096
    temperature: 0.7
    best_for: ["ä¸€èˆ¬èŠå¤©", "å¿«é€Ÿä»»åŠ¡"]
    
  mistral-large:
    base_url: "https://api.mistral.ai/v1"
    max_tokens: 32000
    temperature: 0.4
    best_for: ["é•¿ä¸Šä¸‹æ–‡", "åˆ†æ"]
    
  deepseek-chat:
    base_url: "https://api.deepseek.com/v1"
    max_tokens: 8192
    temperature: 0.2
    best_for: ["ç¼–ç¨‹", "æ•°å­¦"]

# æä¾›å•†é…ç½®
providers:
  openai:
    base_url: "https://api.openai.com/v1"
    models: ["gpt-4", "gpt-3.5-turbo", "gpt-4-turbo"]
    rate_limit: 1000
    
  openrouter:
    base_url: "https://openrouter.ai/api/v1"
    models: ["mistralai/mistral-large", "meta-llama/llama-2-70b-chat"]
    rate_limit: 500
    
  deepseek:
    base_url: "https://api.deepseek.com/v1"
    models: ["deepseek-chat", "deepseek-coder"]
    rate_limit: 100
```

### å·¥å…·é…ç½® (config/tools.yaml)

```yaml
# å·¥å…·ç³»ç»Ÿé…ç½®

tools:
  auto_discovery: true
  directories:
    - "./agents/tool"
    - "./custom_tools"
  
  execution:
    timeout: 30
    max_concurrent: 5
    retry_count: 2
    rate_limit: 100
  
  mcp_servers:
    config_path: "./mcp_servers/mcp_setting.json"
    auto_connect: true
    timeout: 10
    
    servers:
      weather:
        command: "python -m mcp_servers.weather_server"
        port: 8001
        enabled: true
        
      database:
        command: "python -m mcp_servers.database_server"
        port: 8002
        enabled: false
```

## ğŸ–¥ï¸ å‘½ä»¤è¡Œå‚æ•°

### åŸºæœ¬ç”¨æ³•

```bash
python examples/sage_demo.py [é€‰é¡¹]
```

### å¯ç”¨å‚æ•°

| å‚æ•° | ç±»å‹ | æè¿° | ç¤ºä¾‹ |
|------|------|------|------|
| `--api_key` | string | OpenAI API å¯†é’¥ | `--api_key sk-...` |
| `--model` | string | æ¨¡å‹åç§° | `--model gpt-4` |
| `--base_url` | string | API åŸºç¡€ URL | `--base_url https://api.openai.com/v1` |
| `--max_tokens` | integer | æœ€å¤§æ ‡è®°æ•° | `--max_tokens 8192` |
| `--temperature` | float | æ¨¡å‹æ¸©åº¦ | `--temperature 0.3` |
| `--tools_folders` | list | å·¥å…·ç›®å½• | `--tools_folders ./tools ./custom` |
| `--config_file` | string | é…ç½®æ–‡ä»¶è·¯å¾„ | `--config_file ./my_config.yaml` |
| `--debug` | boolean | å¯ç”¨è°ƒè¯•æ¨¡å¼ | `--debug` |
| `--streaming` | boolean | å¯ç”¨æµå¼è¾“å‡º | `--streaming` |
| `--web` | boolean | å¯åŠ¨ç½‘é¡µç•Œé¢ | `--web` |
| `--port` | integer | ç½‘é¡µç•Œé¢ç«¯å£ | `--port 8501` |

### ç¤ºä¾‹å‘½ä»¤

```bash
# åŸºæœ¬ç”¨æ³•
python examples/sage_demo.py --api_key sk-your-key

# é«˜çº§é…ç½®
python examples/sage_demo.py \
  --api_key sk-your-key \
  --model gpt-4 \
  --max_tokens 8192 \
  --temperature 0.3 \
  --debug \
  --streaming

# è‡ªå®šä¹‰å·¥å…·å’Œé…ç½®
python examples/sage_demo.py \
  --api_key sk-your-key \
  --tools_folders ./my_tools ./shared_tools \
  --config_file ./my_config.yaml \
  --web --port 8080

# OpenRouter ä½¿ç”¨
python examples/sage_demo.py \
  --api_key sk-or-v1-your-key \
  --base_url https://openrouter.ai/api/v1 \
  --model mistralai/mistral-large
```

## âš™ï¸ è¿è¡Œæ—¶é…ç½®

### åŠ¨æ€é…ç½®æ›´æ–°

```python
from agents.config import Settings, update_settings

# æ›´æ–°é…ç½®
update_settings(
    debug=True,
    max_iterations=15,
    temperature=0.2
)

# è·å–å½“å‰é…ç½®
settings = Settings()
print(f"å½“å‰æ¨¡å‹: {settings.model.name}")
print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {settings.agent.max_iterations}")
```

### é…ç½®éªŒè¯

```python
from agents.config import validate_config

# éªŒè¯é…ç½®å­—å…¸
config = {
    "model": {"name": "gpt-4", "max_tokens": 8192},
    "agent": {"max_iterations": 10},
    "tool": {"timeout": 30}
}

is_valid, errors = validate_config(config)
if not is_valid:
    print("é…ç½®é”™è¯¯:", errors)
```

### çƒ­é‡è½½é…ç½®

```python
from agents.config import reload_config

# çƒ­é‡è½½é…ç½®æ–‡ä»¶
reload_config("./config/settings.yaml")

# ç›‘å¬é…ç½®æ–‡ä»¶å˜åŒ–
from agents.config import watch_config_file

watch_config_file("./config/settings.yaml", auto_reload=True)
```

## ğŸ¤– æ¨¡å‹é…ç½®

### æ”¯æŒçš„æ¨¡å‹

```python
# OpenAI æ¨¡å‹
"gpt-4"
"gpt-4-turbo"
"gpt-3.5-turbo"

# OpenRouter æ¨¡å‹
"mistralai/mistral-large"
"meta-llama/llama-2-70b-chat"
"anthropic/claude-3-sonnet"

# DeepSeek æ¨¡å‹
"deepseek-chat"
"deepseek-coder"

# æœ¬åœ°æ¨¡å‹ï¼ˆé€šè¿‡ Ollamaï¼‰
"llama2:7b"
"mistral:7b"
```

### æ¨¡å‹ç‰¹å®šè®¾ç½®

```python
model_configs = {
    "gpt-4": {
        "max_tokens": 8192,
        "temperature": 0.3,
        "top_p": 0.9,
        "streaming": True
    },
    "mistral-large": {
        "max_tokens": 32000,
        "temperature": 0.4,
        "stop_sequences": ["Human:", "Assistant:"]
    }
}
```

### è‡ªå®šä¹‰æ¨¡å‹æä¾›å•†

```python
from agents.config import register_provider

# æ³¨å†Œè‡ªå®šä¹‰æä¾›å•†
register_provider(
    name="custom_provider",
    base_url="https://api.custom.com/v1",
    api_key_env="CUSTOM_API_KEY",
    models=["custom-model-1", "custom-model-2"]
)
```

## ğŸ¤– æ™ºèƒ½ä½“é…ç½®

### æ™ºèƒ½ä½“è¡Œä¸ºè®¾ç½®

```yaml
agent:
  task_analysis:
    enabled: true
    max_depth: 3
    complexity_threshold: 0.7
    
  planning:
    enabled: true
    max_subtasks: 20
    parallel_execution: true
    dependency_tracking: true
    
  execution:
    parallel_tools: true
    max_concurrent: 5
    timeout: 300
    retry_failed_tools: true
    
  observation:
    enabled: true
    feedback_threshold: 0.8
    completion_criteria: "all_objectives_met"
    
  summary:
    enabled: true
    include_steps: true
    include_metrics: true
    format: "structured"
```

### è‡ªå®šä¹‰æ™ºèƒ½ä½“é…ç½®

```python
from agents.config import register_agent

# æ³¨å†Œè‡ªå®šä¹‰æ™ºèƒ½ä½“
register_agent(
    name="custom_researcher",
    class_path="my_agents.CustomResearchAgent",
    config={
        "max_research_depth": 5,
        "sources": ["academic", "web", "internal"],
        "output_format": "report"
    }
)
```

## ğŸ› ï¸ å·¥å…·é…ç½®

### å·¥å…·å‘ç°å’Œæ³¨å†Œ

```yaml
tool:
  auto_discovery: true
  directories:
    - "./agents/tool"
    - "./custom_tools"
    - "~/shared_tools"
  
  filters:
    include_patterns: ["*.py", "*.json"]
    exclude_patterns: ["*test*", "*demo*"]
    
  validation:
    strict_typing: true
    require_docstrings: true
    check_dependencies: true
```

### MCP æœåŠ¡å™¨é…ç½®

```yaml
mcp_servers:
  config_path: "./mcp_servers/mcp_setting.json"
  auto_connect: true
  connection_timeout: 10
  request_timeout: 30
  
  servers:
    weather:
      command: "python -m mcp_servers.weather_server"
      port: 8001
      enabled: true
      health_check: "/health"
      
    database:
      command: "node mcp_servers/database_server.js"
      port: 8002
      enabled: false
      environment:
        DB_HOST: "localhost"
        DB_PORT: "5432"
```

### å·¥å…·å®‰å…¨è®¾ç½®

```yaml
tool_security:
  sandbox_mode: true
  allowed_modules:
    - "requests"
    - "json"
    - "datetime"
  blocked_modules:
    - "os"
    - "subprocess"
    - "sys"
  
  resource_limits:
    max_memory: "256MB"
    max_execution_time: 30
    max_file_size: "10MB"
```

## ğŸ”§ é«˜çº§è®¾ç½®

### æ€§èƒ½è°ƒä¼˜

```yaml
performance:
  caching:
    enabled: true
    ttl: 3600
    max_size: "100MB"
    
  memory:
    max_history_length: 100
    cleanup_interval: 300
    gc_threshold: 0.8
    
  concurrency:
    max_workers: 10
    thread_pool_size: 20
    async_enabled: true
```

### å®‰å…¨é…ç½®

```yaml
security:
  api_key_rotation: true
  request_signing: false
  rate_limiting:
    enabled: true
    requests_per_minute: 60
    burst_limit: 10
    
  input_validation:
    max_input_length: 10000
    sanitize_html: true
    block_code_execution: true
```

### ç›‘æ§å’Œæ—¥å¿—

```yaml
monitoring:
  metrics:
    enabled: true
    export_interval: 60
    include_system_metrics: true
    
  alerts:
    error_rate_threshold: 0.1
    response_time_threshold: 5.0
    email_notifications: true
    
logging:
  level: "INFO"
  format: "json"
  file: "./logs/sage.log"
  rotation:
    max_size: "10MB"
    backup_count: 5
    
  structured_logging: true
  include_context: true
  sensitive_data_filtering: true
```

### å¼€å‘é…ç½®

```yaml
development:
  hot_reload: true
  auto_save_conversations: true
  debug_toolbar: true
  
  testing:
    mock_external_apis: true
    test_data_path: "./test_data"
    coverage_reporting: true
    
  profiling:
    enabled: false
    output_path: "./profiles"
    memory_profiling: false
```

## ğŸ“± ç¯å¢ƒç‰¹å®šé…ç½®

### å¼€å‘ç¯å¢ƒ

```yaml
# config/development.yaml
environment: development
debug: true
logging:
  level: DEBUG
model:
  temperature: 0.8
agent:
  max_iterations: 5
```

### ç”Ÿäº§ç¯å¢ƒ

```yaml
# config/production.yaml
environment: production
debug: false
logging:
  level: INFO
  file: "/var/log/sage/sage.log"
security:
  rate_limiting:
    enabled: true
performance:
  caching:
    enabled: true
```

### æµ‹è¯•ç¯å¢ƒ

```yaml
# config/testing.yaml
environment: testing
debug: true
model:
  name: "gpt-3.5-turbo"  # ä½¿ç”¨è¾ƒä¾¿å®œçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
  max_tokens: 1000
testing:
  mock_external_apis: true
```

## ğŸ” é…ç½®æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **é…ç½®ä¸ç”Ÿæ•ˆ**
   ```bash
   # æ£€æŸ¥é…ç½®ä¼˜å…ˆçº§
   python -c "from agents.config import get_settings; print(get_settings())"
   ```

2. **ç¯å¢ƒå˜é‡æœªè¯†åˆ«**
   ```bash
   # éªŒè¯ç¯å¢ƒå˜é‡
   env | grep SAGE_
   ```

3. **é…ç½®æ–‡ä»¶é”™è¯¯**
   ```bash
   # éªŒè¯YAMLè¯­æ³•
   python -c "import yaml; yaml.safe_load(open('config/settings.yaml'))"
   ```

### è°ƒè¯•é…ç½®

```python
from agents.config import debug_config

# æ‰“å°å®Œæ•´é…ç½®
debug_config()

# æ£€æŸ¥é…ç½®æ¥æº
from agents.config import get_config_sources
sources = get_config_sources()
print("é…ç½®æ¥æº:", sources)
```

è¿™ä»½å®Œæ•´çš„é…ç½®å‚è€ƒåº”è¯¥èƒ½å¸®åŠ©æ‚¨å……åˆ†å®šåˆ¶ Sage å¤šæ™ºèƒ½ä½“æ¡†æ¶ä»¥æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚ 